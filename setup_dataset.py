#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†è‡ªåŠ¨å‡†å¤‡è„šæœ¬
è‡ªåŠ¨è§£å‹å’Œç»„ç»‡NUAA-SIRSTå’ŒNUDT-SIRSTæ•°æ®é›†æ–‡ä»¶ç»“æ„
"""

import os
import zipfile
import shutil
from pathlib import Path

def check_dataset_files():
    """æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶...")
    
    expected_files = [
        "NUAA-SIRST.zip",
        "NUDT-SIRST.zip"
    ]
    
    existing_files = []
    missing_files = []
    
    for file_name in expected_files:
        if os.path.exists(file_name):
            existing_files.append(file_name)
            print(f"   æ‰¾åˆ°: {file_name}")
        else:
            missing_files.append(file_name)
            print(f"   ç¼ºå¤±: {file_name}")
    
    return existing_files, missing_files

def create_dataset_structure():
    """åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„"""
    print("\n åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„...")
    
    # åˆ›å»ºä¸»æ•°æ®é›†ç›®å½•
    dataset_dir = Path("dataset")
    dataset_dir.mkdir(exist_ok=True)
    print(f"   åˆ›å»ºç›®å½•: {dataset_dir}")
    
    # åˆ›å»ºå„æ•°æ®é›†å­ç›®å½•
    datasets = ["NUAA-SIRST", "NUDT-SIRST"]
    
    for dataset_name in datasets:
        dataset_path = dataset_dir / dataset_name
        dataset_path.mkdir(exist_ok=True)
        
        # åˆ›å»ºå¿…è¦çš„å­ç›®å½•
        subdirs = ["images", "masks", "50_50", "value_result"]
        for subdir in subdirs:
            (dataset_path / subdir).mkdir(exist_ok=True)
        
        print(f"   åˆ›å»ºæ•°æ®é›†ç›®å½•: {dataset_path}")

def extract_dataset(zip_file, dataset_name):
    """è§£å‹æ•°æ®é›†æ–‡ä»¶"""
    print(f"\nğŸ“¦ è§£å‹ {zip_file} ...")
    
    try:
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            # è§£å‹åˆ°ä¸´æ—¶ç›®å½•
            temp_dir = f"temp_{dataset_name}"
            zip_ref.extractall(temp_dir)
            print(f"   è§£å‹å®Œæˆ: {zip_file}")
            
            # æŸ¥æ‰¾è§£å‹åçš„å®é™…ç›®å½•ç»“æ„
            temp_path = Path(temp_dir)
            extracted_dirs = list(temp_path.iterdir())
            
            if len(extracted_dirs) == 1 and extracted_dirs[0].is_dir():
                # å¦‚æœè§£å‹ååªæœ‰ä¸€ä¸ªç›®å½•ï¼Œè¿›å…¥è¯¥ç›®å½•
                source_dir = extracted_dirs[0]
            else:
                # å¦‚æœè§£å‹åæ˜¯å¤šä¸ªæ–‡ä»¶/ç›®å½•ï¼Œä½¿ç”¨temp_dirä½œä¸ºæºç›®å½•
                source_dir = temp_path
            
            return source_dir, temp_dir
            
    except zipfile.BadZipFile:
        print(f"   é”™è¯¯: {zip_file} ä¸æ˜¯æœ‰æ•ˆçš„ZIPæ–‡ä»¶")
        return None, None
    except Exception as e:
        print(f"   è§£å‹å¤±è´¥: {e}")
        return None, None

def organize_dataset_files(source_dir, dataset_name):
    """æ•´ç†æ•°æ®é›†æ–‡ä»¶åˆ°æ ‡å‡†ç»“æ„"""
    print(f"\nğŸ”§ æ•´ç† {dataset_name} æ•°æ®é›†æ–‡ä»¶...")
    
    source_path = Path(source_dir)
    target_path = Path("dataset") / dataset_name
    
    # æŸ¥æ‰¾imageså’Œmasksç›®å½•
    images_source = None
    masks_source = None
    split_source = None
    
    # é€’å½’æŸ¥æ‰¾imageså’Œmasksç›®å½•
    for path in source_path.rglob("*"):
        if path.is_dir():
            if path.name.lower() == "images":
                images_source = path
            elif path.name.lower() == "masks":
                masks_source = path
            elif path.name in ["50_50", "split"] or "split" in path.name.lower():
                split_source = path
    
    # å¤åˆ¶imagesç›®å½•
    if images_source and images_source.exists():
        target_images = target_path / "images"
        if target_images.exists():
            shutil.rmtree(target_images)
        shutil.copytree(images_source, target_images)
        print(f"   å¤åˆ¶imagesç›®å½•: {len(list(target_images.glob('*')))} ä¸ªæ–‡ä»¶")
    else:
        print(f"   æœªæ‰¾åˆ°imagesç›®å½•")
    
    # å¤åˆ¶masksç›®å½•  
    if masks_source and masks_source.exists():
        target_masks = target_path / "masks"
        if target_masks.exists():
            shutil.rmtree(target_masks)
        shutil.copytree(masks_source, target_masks)
        print(f"   å¤åˆ¶masksç›®å½•: {len(list(target_masks.glob('*')))} ä¸ªæ–‡ä»¶")
    else:
        print(f"   æœªæ‰¾åˆ°masksç›®å½•")
    
    # å¤åˆ¶æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶
    if split_source and split_source.exists():
        target_split = target_path / "50_50"
        if target_split.exists():
            shutil.rmtree(target_split)
        shutil.copytree(split_source, target_split)
        print(f"   å¤åˆ¶æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶")
    else:
        # æŸ¥æ‰¾train.txtå’Œtest.txtæ–‡ä»¶
        train_txt = None
        test_txt = None
        for path in source_path.rglob("*.txt"):
            if "train" in path.name.lower():
                train_txt = path
            elif "test" in path.name.lower():
                test_txt = path
        
        if train_txt and test_txt:
            target_split = target_path / "50_50"
            target_split.mkdir(exist_ok=True)
            shutil.copy2(train_txt, target_split / "train.txt")
            shutil.copy2(test_txt, target_split / "test.txt")
            print(f"   å¤åˆ¶æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶")
        else:
            print(f"    æœªæ‰¾åˆ°æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶ (train.txt, test.txt)")

def cleanup_temp_files(temp_dirs):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    print("\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    for temp_dir in temp_dirs:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            print(f"   åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_dir}")

def verify_dataset_structure():
    """éªŒè¯æ•°æ®é›†ç»“æ„æ˜¯å¦æ­£ç¡®"""
    print("\n éªŒè¯æ•°æ®é›†ç»“æ„...")
    
    datasets = ["NUAA-SIRST", "NUDT-SIRST"]
    all_good = True
    
    for dataset_name in datasets:
        print(f"\n  æ£€æŸ¥ {dataset_name}:")
        dataset_path = Path("dataset") / dataset_name
        
        # æ£€æŸ¥å¿…è¦ç›®å½•
        required_dirs = ["images", "masks", "50_50", "value_result"]
        for dirname in required_dirs:
            dir_path = dataset_path / dirname
            if dir_path.exists():
                if dirname == "images":
                    file_count = len(list(dir_path.glob("*.png")))
                    print(f"     {dirname}/ ({file_count} å¼ å›¾åƒ)")
                elif dirname == "masks":
                    file_count = len(list(dir_path.glob("*.png")))
                    print(f"     {dirname}/ ({file_count} ä¸ªæ ‡ç­¾)")
                elif dirname == "50_50":
                    train_txt = dir_path / "train.txt"
                    test_txt = dir_path / "test.txt"
                    if train_txt.exists() and test_txt.exists():
                        print(f"     {dirname}/ (train.txt, test.txt)")
                    else:
                        print(f"     {dirname}/ (ç¼ºå°‘train.txtæˆ–test.txt)")
                        all_good = False
                else:
                    print(f"     {dirname}/")
            else:
                print(f"     {dirname}/ (ç›®å½•ä¸å­˜åœ¨)")
                all_good = False
    
    return all_good

def print_download_instructions():
    """æ‰“å°ä¸‹è½½è¯´æ˜"""
    print("\nğŸ“¥ æ•°æ®é›†ä¸‹è½½è¯´æ˜:")
    print("=" * 50)
    print("è¯·æ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹æ•°æ®é›†å¹¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•:")
    print("ğŸ“ NUDT-SIRSTæ•°æ®é›†:")
    print("  é“¾æ¥: https://pan.quark.cn/s/c87c1148de39?pwd=AQtj")
    print("  æå–ç : AQtj")
    print()
    print("ğŸ“ NUAA-SIRSTæ•°æ®é›†:")
    print("  é“¾æ¥: https://pan.quark.cn/s/55066db3363d?pwd=DVb1")  
    print("  æå–ç : DVb1")
    print()
    print("ä¸‹è½½å®Œæˆåï¼Œé‡æ–°è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œè‡ªåŠ¨è§£å‹å’Œæ•´ç†ã€‚")

def main():
    """ä¸»å‡½æ•°"""
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    existing_files, missing_files = check_dataset_files()
    
    if missing_files:
        print_download_instructions()
        return
    
    # åˆ›å»ºç›®å½•ç»“æ„
    create_dataset_structure()
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    temp_dirs = []
    
    for zip_file in existing_files:
        dataset_name = zip_file.replace(".zip", "")
        
        # è§£å‹æ•°æ®é›†
        source_dir, temp_dir = extract_dataset(zip_file, dataset_name)
        if source_dir is None:
            continue
            
        temp_dirs.append(temp_dir)
        
        # æ•´ç†æ–‡ä»¶ç»“æ„
        organize_dataset_files(source_dir, dataset_name)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    cleanup_temp_files(temp_dirs)
    
    # éªŒè¯ç»“æ„
    success = verify_dataset_structure()
    
    # æœ€ç»ˆæç¤º
    if success:
        print("æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    else:
        print("æ®é›†å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")

if __name__ == "__main__":
    main() 
